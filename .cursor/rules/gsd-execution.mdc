---
description: GSD-style execution — context from .planning/, XML plans, wave execution, atomic commits
alwaysApply: true
---

# GSD execution discipline

When doing **project** or **phase** work (e.g. "new project", "plan phase N", "execute phase N", "quick task"), follow this discipline so the agent delivers consistent, verifiable work without context rot.

## Working Backwards (start with the end in mind)

Amazon's Working Backwards mechanism: **write the PRFAQ before building**. The PRFAQ is the "end in mind" — requirements and phases must be traceable to the customer outcome it describes.

- **new-project / new-milestone (changing direction):** Before writing REQUIREMENTS or ROADMAP, ensure `.planning/PRFAQ.md` exists and is current. Answer the five questions (who is the customer, what is their problem/opportunity, what is the most critical benefit, how do we know what they want, what does the experience look like), then write a future-oriented press release and FAQ. Derive REQUIREMENTS and ROADMAP from the PRFAQ.
- **plan-phase / execute-phase:** If `.planning/PRFAQ.md` exists, read it. Ensure phase goals and tasks advance the customer outcome and benefits it describes. Flag anything that is out of scope or off-mission.
- **Iteration:** For small iterations, only a PR update or FAQ update may be needed — not a full rewrite.

## MECE Principle (Mutually Exclusive, Collectively Exhaustive)

Every decomposition in the planning hierarchy — PRFAQ → requirements → phases → tasks — must be **MECE**:

- **Mutually Exclusive:** No two items at the same level describe overlapping work. Each requirement, phase, and task owns a distinct concern. If two items touch the same file or feature for the same reason, merge or split them.
- **Collectively Exhaustive:** Items at each level must fully cover the scope defined above them. Requirements must cover all PRFAQ outcomes. Phases must deliver all requirements. Tasks within a phase must deliver the full phase goal — no deliverable is left unassigned.

**Apply MECE at each planning step:**

- **PRFAQ:** The five questions form a MECE set — each addresses a distinct dimension (customer, problem, benefit, evidence, experience). Ensure no dimension is duplicated or omitted.
- **REQUIREMENTS.md:** Group requirements into named, non-overlapping categories (e.g., Data Model, API, Auth, UI, Observability). Verify each requirement belongs to exactly one category and no category is missing from the scope.
- **ROADMAP.md phases:** Each phase has a distinct, non-overlapping outcome. Together they deliver the full product. Flag any phase whose scope bleeds into another.
- **Phase tasks (XML plans):** Each `<task>` owns a distinct concern. No two tasks in the same wave modify the same file for the same reason. Together they must fully implement the phase goal. **Before finalizing a plan, run a MECE check:** (1) scan `<files>` across all tasks for unintended overlap, (2) verify every phase deliverable is assigned to exactly one task.

## Context

- **Before planning or executing:** Read from `.planning/`:
  - `PRFAQ.md` (if present — the "end in mind")
  - `PROJECT.md`, `REQUIREMENTS.md`, `ROADMAP.md`, `STATE.md`
  - For a given phase: `phases/XX-phase-slug/CONTEXT.md` and `phases/XX-phase-slug/RESEARCH.md` (if present)
- Keep context size in mind; do not overload the session. Prefer referencing these files over re-pasting large blocks.

## Plans

- **Phase plans:** Write plans as **XML tasks** in `phases/XX-phase-slug/` with filenames `XX-01-PLAN.md`, `XX-02-PLAN.md`, etc.
- **Task format (required):**
  ```xml
  <task type="auto">
    <name>Short task name</name>
    <files>path/to/file1.ext path/to/file2.ext</files>
    <action>
      Clear instructions: what to implement, constraints, patterns to use.
    </action>
    <verify>How to verify (e.g. command or check)</verify>
    <done>One-line success criterion</done>
  </task>
  ```
- **Scope:** Keep plans small — typically 2–3 tasks per phase so each task fits in a fresh context window.

## Execution (execute-phase)

- **Discover:** Find all `*-PLAN.md` in the phase folder (e.g. `phases/01-setup/`).
- **Dependencies:** Infer dependencies from file paths or explicit references. Group plans into **waves**: independent plans in the same wave, dependent plans in later waves.
- **Run waves:** For each wave, run **independent** plans in parallel (e.g. via `mcp_task` subagents, one plan per subagent). Run waves **sequentially**.
- **After each completed task:**
  - Create or update the corresponding `*-SUMMARY.md` (what was done, what changed).
  - **Commit that task only** — one atomic commit per task. Message format e.g. `feat(01-01): short description`.
- **After all waves:** Run verifier (if configured): check codebase against phase goals; write `phases/XX-phase-slug/VERIFICATION.md`.

## Interactive Debug Notebooks (Databricks)

When generating or modifying Databricks notebooks that run in a job or pipeline (the numbered notebooks in `notebooks/`), follow this debug-first workflow. Notebooks executed headlessly in jobs are hard to debug — schema mismatches, `ai_query()` response structure surprises, and Spark SQL edge cases surface only at runtime.

### When to generate a debug notebook

- Whenever you create or substantially modify a numbered pipeline notebook in `notebooks/` (`00_*.py` through `05_*.py` or higher).
- Whenever a job run fails and the user asks you to investigate or fix the failing notebook.
- Do **not** generate debug notebooks for source modules (`src/`), web app files (`src/app/`), or config-only notebooks that have no runtime logic.

### Debug notebook conventions

1. **Naming:** `XX_Interactive_Debug.py` where `XX` matches the production notebook number (e.g., `02_Interactive_Debug.py` for `02_Build_Knowledge_Graph.py`). Debug notebooks live in `notebooks/spikes/`.
2. **Self-contained:** Inline all configuration — copy the `config` dict values directly instead of using `%run ../src/config`. The notebook must run with zero `%run` dependencies so the user can execute it on any cluster without syncing the full repo.
3. **Header cell:** First markdown cell must say: `# XX — <Title> (Interactive Debug)` followed by: *"Self-contained notebook for interactive debugging. Run cell by cell. Once everything works, update the production notebook in `notebooks/` with the fixes."*
4. **Diagnostic cells:** After each substantive operation, add a cell titled `DEBUG: <what it inspects>`. Include:
   - `printSchema()` for any DataFrame produced by `ai_query()` or complex struct operations.
   - `display(df.limit(2))` to show sample data at intermediate steps.
   - `try/except` probes when the field access path is uncertain (e.g., `extracted.result` vs `extracted.result.result`).
5. **Mirror structure:** Keep the same logical steps and section headers as the production notebook so diffs are easy to follow.
6. **No shared-state side effects:** Debug notebooks may write to the same tables as production notebooks (they share config), but clearly comment which cells are destructive writes vs read-only inspection.

### Incorporate-back flow

When the user reports that the debug notebook runs cleanly and fixes are ready:

1. **Read** the debug notebook from `notebooks/spikes/XX_Interactive_Debug.py`.
2. **Diff** against the production notebook in `notebooks/XX_<Name>.py` — identify substantive logic changes vs diagnostic scaffolding.
3. **Apply** only the logic fixes to the production notebook, preserving its clean structure: `%run` for shared config, no debug/diagnostic cells.
4. **Verify** the production notebook's logic matches the working debug notebook.
5. **Do not delete** the debug notebook — it remains in the repo as a runnable reference. Update it if the production notebook changes significantly.

### File layout addition

- Production notebooks live in `notebooks/`. Debug/spike notebooks live in `notebooks/spikes/`.

## Interactive Verification Gate

Before committing any notebook or `src/` module that runs on Databricks:

1. **SQL validation:** Every SQL string (in `spark.sql()`, `ai_query()`, or raw SQL) must be tested against the target runtime. Common traps:
   - `COLLECT_LIST(expr ORDER BY ...)` is NOT valid Spark SQL — use `ARRAY_SORT(COLLECT_LIST(STRUCT(...)))` pattern
   - `ai_query()` `responseFormat` struct syntax varies by runtime version
   - String escaping in f-string SQL (single quotes inside CONCAT)

2. **API contract check:** When using `mlflow.genai.evaluate()`, `mlflow.search_traces()`, or any SDK function, verify the call signature against the skill reference (`.cursor/skills/databricks-mlflow-evaluation/references/CRITICAL-interfaces.md` and `GOTCHAS.md`). Common traps:
   - `evaluate()` creates its own run — do not nest inside `start_run()`
   - `evaluate()` creates its own traces — do not add `@mlflow.trace` to predict functions
   - `search_traces()` uses `experiment_ids` + `filter_string`, not `run_id`

3. **Spike-first for external calls:** Any notebook cell that calls an external service (LLM endpoint, embedding endpoint, Model Serving) must first be validated in a spike notebook with a single-item test before running on the full dataset.

## Verification

- After execution, update `VERIFICATION.md` in the phase folder with results against phase goals.
- **MECE audit (include in every VERIFICATION.md):**
  - *Mutual exclusivity:* Did any two tasks duplicate work or modify the same file for the same reason? List any overlaps found.
  - *Collective exhaustiveness:* Is every phase goal covered by at least one completed task? List any goals with no corresponding task output.
- If the user runs verify-work (UAT): list testable deliverables, walk through yes/no or issues, optionally spawn a debug subagent for failures and produce fix plans for re-execution.

## Quick mode

- For ad-hoc tasks: Create `.planning/quick/NNN-short-slug/PLAN.md` using the **same XML task format** (single task).
- Execute the task, write `SUMMARY.md` in that folder, then **one atomic commit**.

## File layout reference

- Root: `.planning/PRFAQ.md` (end in mind), `PROJECT.md`, `REQUIREMENTS.md`, `ROADMAP.md`, `STATE.md`, `config.json`
- Phase: `.planning/phases/XX-slug/CONTEXT.md`, `RESEARCH.md`, `XX-01-PLAN.md`, `XX-01-SUMMARY.md`, `VERIFICATION.md`
- Quick: `.planning/quick/NNN-desc/PLAN.md`, `SUMMARY.md`

---

## Drucker Discipline (Learning Loops)

### Hard Gates (These are REQUIREMENTS, not suggestions)

**Before Phase N+1 planning can begin:**
1. Phase N's `RETROSPECTIVE.md` MUST exist in `.planning/phases/[phase-N-folder]/`
2. Phase N+1's `CONTRIBUTION-GATE.md` MUST be completed and approved (all 6 questions = YES)
3. Phase N+1 planning MUST include a section: "Adjustments from Phase N Retrospective: [list specific changes being applied]"

If any of these are missing, STOP and create them before proceeding.

**After each phase completes (before the commit):**
1. Verifier runs verification → VERIFICATION.md
2. Evaluator runs evaluation → EVAL-REPORT.md
3. Main agent writes RETROSPECTIVE.md using specialist observations + own judgment
4. Main agent updates relevant decision files in `.planning/decisions/` with validation evidence
5. Main agent decides fidelity promotions based on Evaluator recommendations
6. All Drucker artifacts are included in the phase completion commit

**For major decisions during any phase:**
1. Create a new decision file: `.planning/decisions/D-NNN-short-slug.md`
2. Use the template from `.planning/templates/DECISION-TEMPLATE.md`
3. Classify reversibility (one-way door / two-way door)
4. Record implementation fidelity level
5. Red flag check: one-way door + Level 1 fidelity = stop and reconsider
6. Include platform leverage check
7. Update `.planning/decisions/README.md` index

### Fidelity Level Policy

**Three fidelity levels govern how code is written and where it lives:**

| Level | Name | Where It Lives | Purpose | Standards |
|-------|------|---------------|---------|-----------|
| 1 | Prove It | `notebooks/spikes/` | Answer: "Can this work?" | Hardcoded OK, no tests required, speed priority |
| 2 | Shape It | `src/` modules | Answer: "Is this maintainable?" | Clean interfaces, config externalized, tested |
| 3 | Harden It | `src/` + `deploy/` | Answer: "Is this production-ready?" | Error handling, retry, observability, security |

**Promotion rules:**
- Fidelity promotion is an EXPLICIT act — a planned task in the phase plan, not a gradual drift
- Level 1 → Level 2: Extract proven logic from spike notebook into `src/` modules. Write tests.
- Level 2 → Level 3: Add error handling, retry logic, observability. Comprehensive testing.
- NEVER skip levels. Code goes 1 → 2 → 3, not 1 → 3.
- Spike notebooks (`notebooks/spikes/`) older than 2 phases without promotion or deletion must be addressed in the current phase's Retrospective.

**Notebook taxonomy:**
- `notebooks/spikes/` — Level 1 exploration. Temporary. Named with phase prefix (e.g., `04_agent_spike.py`).
- `notebooks/XX_name.py` — Level 2 walkthrough notebooks. Numbered, maintained, demo-ready. These call into `src/` modules.
- Debug notebooks — NOT committed. Local throwaway tools for investigating issues.

### Mid-Phase Checkpoint (for phases longer than 1 week)
At the halfway point, the main agent answers:
1. Are we still on track against the phase plan?
2. Any surprises or unplanned work?
3. Anything we'd do differently right now?
4. Are any Level 1 spikes ready for early promotion?

Document answers briefly in `.planning/phases/[current-phase]/CHECKPOINT.md`. This takes 5 minutes.

### Subagent Coordination
When executing phase tasks:
1. Read `.planning/SUBAGENT-COORDINATION.md` for wave structure and file ownership
2. Spawn specialists in wave order: Builder → Verifier → Evaluator
3. Provide each specialist with: task, fidelity level, requirements, prior retrospective
4. Evaluator always runs LAST
5. Use specialist output to inform the Retrospective and fidelity promotion decisions
