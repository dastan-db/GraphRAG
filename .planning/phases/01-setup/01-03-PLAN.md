<task type="auto">
<name>LangGraph agent + query demo + evaluation framework</name>
<files>src/agent/tools.py src/agent/agent.py src/evaluation/evaluation.py src/evaluation/flat_rag.py notebooks/03_Build_Agent.py notebooks/04_Query_Demo.py notebooks/05_Evaluation.py</files>
<action>
Build LangGraph ResponsesAgent with 5 graph traversal tools (find_entity,
find_connections, trace_path, get_context_verses, get_entity_summary) that
query Delta tables via Spark SQL. Create agent notebook that logs to MLflow
and deploys to Model Serving. Build query demo notebook with 6 multi-hop
questions. Build evaluation framework: 20 ground-truth Q&A pairs, governance
scorers (hallucination_check, citation_completeness, provenance_chain),
quality scorers (Correctness, RelevanceToQuery, grounded_reasoning,
multi_hop_reasoning, verse_citation), flat RAG baseline, cost analysis.
</action>
<verify>Agent answers multi-hop questions using tools. Notebook 03 logs model to MLflow. Notebook 04 runs 6 demo queries. Notebook 05 evaluates 4 configurations on 20 questions.</verify>
<done>Agent built and logged; demo runs end-to-end; evaluation framework produces governance and quality metrics.</done>
</task>

<!-- Original plan referenced LangChain GraphCypherQAChain + Neo4j Cypher.
     Pivoted to LangGraph ResponsesAgent + Spark SQL tools.
     This plan updated 2026-02-28 to reflect what was actually built. -->
